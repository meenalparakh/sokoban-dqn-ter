{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from typing import Any\n",
    "from copy import deepcopy\n",
    "# from gym.wrappers import Monitor\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from typing import Any\n",
    "from collections import deque\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(logs, x_key, y_key, legend_key, **kwargs):\n",
    "    nums = len(logs[legend_key].unique())\n",
    "    palette = sns.color_palette(\"hls\", nums)\n",
    "    if 'palette' not in kwargs:\n",
    "        kwargs['palette'] = palette\n",
    "    sns.lineplot(x=x_key, y=y_key, data=logs, hue=legend_key, **kwargs)\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NChain Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NChainEnv(gym.Env):\n",
    "    \"\"\"n-Chain environment\n",
    "\n",
    "    This game presents moves along a linear chain of states, with two actions:\n",
    "     0) forward, which moves along the chain but returns no reward\n",
    "     1) backward, which returns to the beginning and has a small reward\n",
    "\n",
    "    The end of the chain, however, presents a large reward, and by moving\n",
    "    'forward' at the end of the chain this large reward can be repeated.\n",
    "\n",
    "    At each action, there is a small probability that the agent 'slips' and the\n",
    "    opposite transition is instead taken.\n",
    "\n",
    "    The observed state is the current state in the chain (0 to n-1).\n",
    "\n",
    "    This environment is described in section 6.1 of:\n",
    "    A Bayesian Framework for Reinforcement Learning by Malcolm Strens (2000)\n",
    "    http://ceit.aut.ac.ir/~shiry/lecture/machine-learning/papers/BRL-2000.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, n=20, slip=0.0, small=0, large=1, ):\n",
    "        self.n = n\n",
    "        self.slip = slip  # probability of 'slipping' an action\n",
    "        self.small = small  # payout for 'backwards' action\n",
    "        self.large = large  # payout at end of chain for 'forwards' action\n",
    "        self.state = 0  # Start at beginning of the chain\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(self.n)\n",
    "        self.time_step = 0\n",
    "        self.max_time_step = 100\n",
    "        self.info = False\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time_step += 1\n",
    "        assert self.action_space.contains(action)\n",
    "        reward = 0\n",
    "        if np.random.rand() < self.slip:\n",
    "            action = not action  # agent slipped, reverse action taken\n",
    "        if action:  # 'backwards': go back to the beginning, get small reward\n",
    "#             reward = self.small\n",
    "            if(self.state > 0):\n",
    "                self.state = self.state - 1\n",
    "            \n",
    "        else:\n",
    "            assert (self.state < (self.n-1))\n",
    "            self.state = self.state +1\n",
    "#             reward = 1\n",
    "            if(self.state == (self.n-1)):\n",
    "                reward = self.large \n",
    "        \n",
    "        done = (self.state == (self.n-1)) or (self.time_step == self.max_time_step)\n",
    "#         reward = self.large if (self.state == (self.n-1)) else 0\n",
    "        if (self.time_step == self.max_time_step):\n",
    "            self.info = True\n",
    "        return self.state, reward, done, self.info\n",
    "\n",
    "    def reset(self):\n",
    "        self.time_step = 0\n",
    "        self.state = 0\n",
    "        self.info = False\n",
    "        return self.state\n",
    "    \n",
    "#     def action_sample(self):\n",
    "#         return self.action_space.sample()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffers : Cyclic and Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import ReplayBufferGraph, PrioritizedReplayBuffer, CyclicBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QLearningAgent:\n",
    "    env: gym.Env #\n",
    "    learning_rate: float #\n",
    "    gamma: float#\n",
    "    initial_epsilon: float#\n",
    "    min_epsilon: float#\n",
    "    max_decay_episodes: int#\n",
    "    capacity: int#\n",
    "    batch_size: int\n",
    "    warmup_steps: int\n",
    "    init_q_value: float\n",
    "    verbose_buffer: bool\n",
    "    buffer_type: str\n",
    "    eta: float\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.num_states = self.env.observation_space.n\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.reset()\n",
    "        self.ground_truth = np.array([[0.83451376, 0.82616862],\n",
    "                                   [0.84294319, 0.82616862],\n",
    "                                   [0.85145777, 0.83451376],\n",
    "                                   [0.86005835, 0.84294319],\n",
    "                                   [0.86874581, 0.85145777],\n",
    "                                   [0.87752102, 0.86005835],\n",
    "                                   [0.88638487, 0.86874581],\n",
    "                                   [0.89533825, 0.87752102],\n",
    "                                   [0.90438208, 0.88638487],\n",
    "                                   [0.91351725, 0.89533825],\n",
    "                                   [0.92274469, 0.90438208],\n",
    "                                   [0.93206535, 0.91351725],\n",
    "                                   [0.94148015, 0.92274469],\n",
    "                                   [0.95099005, 0.93206535],\n",
    "                                   [0.96059601, 0.94148015],\n",
    "                                   [0.970299  , 0.95099005],\n",
    "                                   [0.9801    , 0.96059601],\n",
    "                                   [0.99      , 0.970299  ],\n",
    "                                   [1.        , 0.9801    ],\n",
    "                                   [0.        , 0.        ]])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        ### TODO: decay epsilon by ep_reduction while respecting min_epsilon ################\n",
    "        ###.      this function is called every episode\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon - self.ep_reduction)\n",
    "\n",
    "        #####################################################################\n",
    "    \n",
    "    def reset(self):\n",
    "#         print(\"eta\", self.eta)\n",
    "        if self.buffer_type == 'cyclic':\n",
    "            self.buffer = CyclicBuffer(self.capacity)\n",
    "        else:\n",
    "            self.buffer = ReplayBufferGraph(max_transitions = self.capacity, \n",
    "                                            verbose = self.verbose_buffer,\n",
    "                                            vertex_dim = 1, \n",
    "                                            state_dim = 1, \n",
    "                                            projection_matrix = np.array([[1]]))\n",
    "            \n",
    "            self.per_buffer = PrioritizedReplayBuffer(self.capacity)\n",
    "            \n",
    "            \n",
    "    \n",
    "        self.epsilon = self.initial_epsilon\n",
    "        self.ep_reduction = (self.epsilon - self.min_epsilon) / float(self.max_decay_episodes)\n",
    "        self.Q = np.random.rand(self.num_states, self.num_actions)*self.init_q_value #np.ones((self.num_states, self.num_actions)) \n",
    "\n",
    "    def update_Q(self):#, state, action, reward, next_state, done):\n",
    "        ### TODO: update self.Q given new experience. #######################\n",
    "\n",
    "        if self.buffer_type == 'cyclic':\n",
    "            if len(self.buffer) < self.warmup_steps:\n",
    "                return False, np.sum(abs(self.ground_truth-self.Q))/40\n",
    "        \n",
    "        else:\n",
    "            if len(self.buffer.batch_queue) < self.batch_size:\n",
    "                return False, np.sum(abs(self.ground_truth-self.Q))/40\n",
    "            if (self.eta > 1e-5) and (len(self.per_buffer) < self.batch_size):\n",
    "                return False, np.sum(abs(self.ground_truth-self.Q))/40\n",
    "        \n",
    "        if self.buffer_type == 'cyclic' or (np.random.rand() >= self.eta):\n",
    "            state, action, reward, next_state, done = self.buffer.sample(self.batch_size)\n",
    "            sampled_per = False\n",
    "        else:\n",
    "            sampled_per = True\n",
    "            state, action, reward, next_state, done, idx, is_weight = self.per_buffer.sample(self.batch_size)\n",
    "\n",
    "        \n",
    "        state = state[0]\n",
    "        action = action[0]\n",
    "        reward = reward[0]\n",
    "        next_state = next_state[0]\n",
    "        done = done[0]\n",
    "        # compute target\n",
    "        y = reward + self.gamma*np.amax(self.Q[next_state,:])\n",
    "        # compute error\n",
    "        e = y - self.Q[state, action]\n",
    "        # update Q\n",
    "        \n",
    "        self.Q[state, action] = self.Q[state, action] + self.learning_rate * e\n",
    "        \n",
    "        if done:\n",
    "            self.Q[next_state, :] = 0\n",
    "            self.Q[state, action] = reward\n",
    "            \n",
    "        if sampled_per:\n",
    "            self.per_buffer.update_priorities(idx, [max(abs(e), 0.001)])\n",
    "            \n",
    "#       print(\"value error\", np.sum(abs(self.ground_truth-self.Q))/40)\n",
    "        return True, np.sum(abs(self.ground_truth-self.Q))/40\n",
    "\n",
    "        #####################################################################\n",
    "    def append_sample_to_per(self, state, action, reward, next_state, done):\n",
    "        self.per_buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    def get_action(self, state = None, choose_random = False, greedy_only = False):\n",
    "        \n",
    "        if (greedy_only):\n",
    "            m = max(self.Q[state, :])\n",
    "            index = [i for i, j in enumerate(self.Q[state, :]) if j == m]\n",
    "#             print(\"index\", index)\n",
    "            best_action = np.random.choice(index)\n",
    "            chosen_action = best_action\n",
    "            return chosen_action\n",
    "\n",
    "\n",
    "        ### TODO: select an action given self.Q and self.epsilon ############\n",
    "        random_num = np.random.uniform(0., 1.)\n",
    "        if choose_random or random_num < self.epsilon:\n",
    "            chosen_action = np.random.choice(np.arange(self.num_actions))\n",
    "#             print(f'randomly chosen action {chosen_action}')\n",
    "        else:\n",
    "            m = max(self.Q[state, :])\n",
    "            index = [i for i, j in enumerate(self.Q[state, :]) if j == m]\n",
    "#             print(\"index\", index)\n",
    "            best_action = np.random.choice(index)\n",
    "            chosen_action = best_action\n",
    "#             print(f'best action {chosen_action}')\n",
    "        return chosen_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QLearningEngine:\n",
    "    env: gym.Env\n",
    "    agent: Any\n",
    "    smooth_len: int\n",
    "    max_episodes: int\n",
    "    max_updates_record: int\n",
    "        \n",
    "    def test(self, env=None, render=False):\n",
    "        rewards = []\n",
    "        actions_outer = []\n",
    "        for i in range(10):\n",
    "            actions = []\n",
    "            env = self.env if env is None else env\n",
    "            ob = env.reset()\n",
    "            ret = 0\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.agent.get_action(ob, greedy_only=True)\n",
    "                actions.append(action)\n",
    "                next_ob, reward, done, info = env.step(action)\n",
    "                ret += reward\n",
    "                ob = next_ob\n",
    "                if done:\n",
    "                    rewards.append(ret)\n",
    "                    actions_outer.append(actions)\n",
    "                    break\n",
    "#         print(actions_outer)\n",
    "        return np.mean(rewards)\n",
    "    \n",
    "    def run(self, n_runs=1):\n",
    "        rewards = []\n",
    "        log = []\n",
    "        log_updates = []\n",
    "                \n",
    "        for i in tqdm(range(n_runs), desc='Runs'):\n",
    "            total_updates = 0\n",
    "            self.agent.reset()\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            smooth_ep_return = deque(maxlen=self.smooth_len)\n",
    "            ep_rewards = []\n",
    "            \n",
    "            ep_current_rewards = []\n",
    "            ep_steps = []\n",
    "            ep_updates = []\n",
    "            \n",
    "            smooth_test_returns = deque(maxlen=self.smooth_len)\n",
    "            test_returns = []\n",
    "            \n",
    "            smooth_value_errors = deque(maxlen=self.smooth_len)\n",
    "            value_errors = []\n",
    "            per_update_value_error = []\n",
    "\n",
    "            ret = 0\n",
    "            num_ep = 0\n",
    "                       \n",
    "            for t in tqdm(range(self.max_episodes), desc='Episode'):\n",
    "                \n",
    "                if t < self.agent.warmup_steps:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = self.agent.get_action(state)\n",
    "                \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                true_done = done and not info\n",
    "                \n",
    "  \n",
    "                if self.agent.buffer_type == 'cyclic':\n",
    "                    self.agent.buffer.append((state, action, reward, next_state, true_done))\n",
    "                    self.agent.update_Q()\n",
    "                    total_updates += 1\n",
    "                else:\n",
    "                    if true_done:\n",
    "                        self.agent.buffer.add_to_terminal_vertices(next_state)\n",
    "                        \n",
    "                    state_array = np.array([[state]])\n",
    "                    next_state_array = np.array([[next_state]])\n",
    "                    self.agent.buffer.append((state_array, action, reward, next_state_array, true_done, t))\n",
    "                    self.agent.per_buffer.append((state_array, action, reward, next_state_array, true_done))\n",
    "#                     self.agent.per_buffer.append((state, action, reward, next_state, true_done))\n",
    "\n",
    "                    ########################################################            \n",
    "                    if (self.agent.eta >= 1) or len(self.agent.buffer.terminal_vertices) > 0:\n",
    "                        if self.agent.eta < 1:\n",
    "                            while (t > self.agent.warmup_steps) and (len(self.agent.buffer.batch_queue) < self.agent.batch_size):\n",
    "                                result = self.agent.buffer.step_reverse_BFS()\n",
    "        #                         print('\\n')\n",
    "                                if not result:\n",
    "                                    break\n",
    "\n",
    "                        if t > self.agent.warmup_steps:\n",
    "                            success, value_error = self.agent.update_Q()\n",
    "                            total_updates += int(success)\n",
    "     \n",
    "                    ########################################################\n",
    "                \n",
    "                ret += reward\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    \n",
    "                    test_ret = self.test()\n",
    "#                         print(f'Step:{t} Testing Return: {test_ret}, Total updates:{total_updates}')\n",
    "                        \n",
    "                    state = self.env.reset()\n",
    "                    ep_current_rewards.append(ret)\n",
    "                    smooth_ep_return.append(ret)\n",
    "                    ep_rewards.append(np.mean(smooth_ep_return))\n",
    "                    ep_steps.append(t)\n",
    "                    ep_updates.append(total_updates)\n",
    "                    smooth_test_returns.append(test_ret)\n",
    "                    test_returns.append(np.mean(smooth_test_returns))\n",
    "                    value_errors.append(np.mean(abs(self.agent.ground_truth-self.agent.Q)))\n",
    "#                     smooth_value_errors.append(value_error)\n",
    "#                     value_errors.append(np.mean(smooth_value_errors))\n",
    "                        \n",
    "                    ret = 0\n",
    "                    num_ep += 1\n",
    "\n",
    "                self.agent.decay_epsilon()\n",
    "                \n",
    "            interp_test_returns = np.interp(np.arange(self.max_episodes), ep_updates, test_returns)\n",
    "            interp_value_errors = np.interp(np.arange(self.max_episodes), ep_updates, value_errors)\n",
    "            run_log2 = pd.DataFrame({'Test return': interp_test_returns[:self.max_updates_record],\n",
    "                                     'Value error': interp_value_errors[:self.max_updates_record],\n",
    "                                     'Number of updates': np.arange(self.max_episodes)[:self.max_updates_record],\n",
    "                                     'buffer_type': self.agent.buffer_type,\n",
    "                                     'iqv': self.agent.init_q_value,\n",
    "                                     'eta':str(self.agent.eta)})\n",
    "            \n",
    "            run_log = pd.DataFrame({'return': ep_rewards, \n",
    "                                    'current_rewards': ep_current_rewards,\n",
    "                                    'steps': ep_steps,\n",
    "                                    'num_updates': ep_updates,\n",
    "                                    'episode': np.arange(len(ep_rewards)), \n",
    "                                    'test_rewards': test_returns,\n",
    "                                    'value_errors': value_errors,\n",
    "                                    'buffer_type': self.agent.buffer_type,\n",
    "                                    'iqv': self.agent.init_q_value,\n",
    "                                    'eta':str(self.agent.eta)})\n",
    "            log.append(run_log)\n",
    "            log_updates.append(run_log2)\n",
    "            \n",
    "            \n",
    "        return log, log_updates\n",
    "\n",
    "    \n",
    "    \n",
    "def qlearning_sweep_eta(init_q_value, buffers = ['ter'], n_runs=4, smooth_len = 10, \n",
    "                    max_episodes=100000, epsilon=0.9, learning_rate=0.99, max_decay_epsiodes = 1000,\n",
    "                    max_updates_record = 100,\n",
    "                    T_warmup = 100, batch_size = 1, capacity = 10000, etas = [0.1]):\n",
    "    logs = dict()\n",
    "    agents = []\n",
    "    seed = 0\n",
    "    env = NChainEnv()    \n",
    "\n",
    "    configs = {'env':env,\n",
    "               'learning_rate':learning_rate,\n",
    "               'gamma':0.99,\n",
    "               'warmup_steps':T_warmup,\n",
    "               'capacity':capacity,\n",
    "               'initial_epsilon':epsilon,\n",
    "               'min_epsilon':0.01,\n",
    "               'batch_size' : batch_size,\n",
    "               'max_decay_episodes':max_decay_epsiodes,\n",
    "               'init_q_value':init_q_value,\n",
    "               'verbose_buffer':False}\n",
    "    \n",
    "    for buffer in buffers:\n",
    "        if buffer == 'ter':\n",
    "            for eta in etas:\n",
    "                set_random_seed(seed=seed)\n",
    "                agent = QLearningAgent(**configs, eta = eta, buffer_type = buffer)\n",
    "                engine = QLearningEngine(env=env, agent=agent, \n",
    "                                         max_episodes=max_episodes, \n",
    "                                         smooth_len = smooth_len,\n",
    "                                        max_updates_record = max_updates_record)\n",
    "                _, ep_log = engine.run(n_runs)\n",
    "                ep_log = pd.concat(ep_log, ignore_index=True)\n",
    "                logs[str(eta)] = ep_log\n",
    "                agents.append(agent)\n",
    "        else:\n",
    "            eta = buffer\n",
    "            set_random_seed(seed=seed)\n",
    "            agent = QLearningAgent(**configs, eta = eta, buffer_type = buffer)\n",
    "            engine = QLearningEngine(env=env, agent=agent, \n",
    "                                     max_episodes=max_episodes, \n",
    "                                     smooth_len = smooth_len,\n",
    "                                    max_updates_record = max_updates_record)\n",
    "            _, ep_log = engine.run(n_runs)\n",
    "            ep_log = pd.concat(ep_log, ignore_index=True)\n",
    "            logs[str(eta)] = ep_log\n",
    "            agents.append(agent)\n",
    "            \n",
    "    logs = pd.concat(logs, ignore_index=True)\n",
    "    \n",
    "    return logs, agents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "buffers = ['ter', 'cyclic']\n",
    "etas = [0.001, 0.1, 0.99]\n",
    "\n",
    "num_plots = 4\n",
    "\n",
    "eps_logs, eps_agents = qlearning_sweep_eta(init_q_value=0.1, \n",
    "                                           n_runs=3,\n",
    "                                           max_updates_record=200,\n",
    "                                           buffers = buffers,\n",
    "                                           max_episodes=5000,\n",
    "                                           max_decay_epsiodes=5000,\n",
    "                                           epsilon=0.99, \n",
    "                                           T_warmup = 40, \n",
    "                                           batch_size = 1, \n",
    "                                           smooth_len = 1,\n",
    "                                           capacity = 1000, etas = etas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(eps_logs, x_key='Number of updates', y_key='Test return', legend_key='eta', estimator='mean', ci='sd', palette = sns.color_palette(\"hls\", 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(eps_logs, x_key='Number of updates', y_key='Value error', legend_key='eta', estimator='mean', ci='sd', palette = sns.color_palette(\"hls\", 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(eps_logs, x_key='episode', y_key='test_rewards', legend_key='eta', estimator='mean', ci='sd', palette = sns.color_palette(\"hls\", 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(eps_logs, x_key='num_updates', y_key='value_errors', legend_key='eta', estimator='mean', ci='sd', palette = sns.color_palette(\"hls\", 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
